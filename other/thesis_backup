%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%             PREAMBLE  !!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%            NECESSARY PACKAGES         

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


%%%      TO DOWNLOAD THE NECESSARY PACKAGES, GO TO "CTAN.ORG"  AND SEARCH FOR PACKAGES   !!!!!!      %%%


\documentclass[12pt,letterpaper]{report}   		%%% specifies document type with additional options %%%
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} 	%%% Customization of margins %%%
\usepackage{appendix}  		%%% Allows you to put appendices in document %%%
\usepackage{pslatex}   	%%% Uses Times New Roman font %%%
\usepackage{graphicx}     %adds options to the \includegraphics command for images and figures
\usepackage{tocvsec2}	%governs section numbering and entries in table of contents
\usepackage[nottoc,notlot,notlof]{tocbibind}	%adds bibliography, and other entries to Table of Contents
\usepackage[labelsep=period,singlelinecheck=false]{caption}   %%% Used to manipulate captions, puts period after figure number, and aligns captions to the left. %%%
\usepackage{tabularx}  		%%% Allows you to construct tables that extend to page margins %%%
\usepackage{blindtext} 		%%% Allows you to enter default text into space %%%
\usepackage{subcaption} 			%%% used to manipulate subcaptions %%%
\usepackage{amsmath}	%package allows use of mathematical symbols, styles of symbols, theorems, etc.
%to align equations to the left, type \usepackage[fleqn]{amsmath}
\usepackage{float}  		%%% Used to fix position of figures and tables %%%
%\usepackage{showframe}  	%%% Allows you to see outline of page layout; this should be turned off in final draft of document %%%

\usepackage{authblk}	%provides option for affiliation of author, as well as footnotes
\usepackage{chngcntr}	 %%% used to modify table and figure numbering
\usepackage[compact]{titlesec} 	%%% Package that helps customize chapters and section styles; compact reduces space above and below titles %%%

\usepackage{tocloft}  	%%% Allows customization of List of Figures/List of Tables/Table of Contents page %%%
\usepackage[nodisplayskipstretch]{setspace}  		%%% Packaged used for setting spacing for document %%%
\setstretch{1.5}%Eliminates excessive spacing between lines of text and floating objects. Also, material in footnotes will remain single-spaced.

\usepackage{etoolbox,lipsum}	%package allows you to use dummy text, toolbox allows alteration of latex commands
\usepackage{fancyhdr}   	%%% Package used to customize headers and footers of document %%%
\usepackage{indentfirst} 		%%% Package indents starting paragraphs in chapters and sections %%%
\usepackage[none]{hyphenat}  		%%% Prevents words from going to the next line (hyphenation) %%%

\usepackage[ruled]{algorithm2e}

\usepackage{amsfonts}  %for mathbb

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%     CHANGES MADE TO CHAPTERS AND SECTIONS   

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

\titleformat{\chapter}{\centering\normalsize\bfseries}{\chaptername\ \thechapter}{0pt}{\vskip .125in\centering}	 %%% Formats chapter and heading %%%
\titlespacing{\chapter}{0pt}{.125in}{.125in}  	%%% Controls spacing above and below of chapter %%%

\titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{} 	%%% makes section titles bold face, roman %%%

\titleformat{\subsection}[runin]{\normalsize\bfseries}{\indent\thesubsection}{1em}{} %%% Formats subsection heading; puts text and subsection title on same line %%%

\titleformat{\subsubsection}[runin]{\normalsize\bfseries\itshape}{\indent\thesubsubsection}{1em}{}	 %Formats subsubsection heading
%\titlespacing{\subsubsection}{0pt}{.0125in}{.0125in}   %Controls spacing above and below subsubsection

\titleformat{\paragraph}[runin]{\normalfont\itshape}{\indent\theparagraph}{1em}{} %was an \hspace{2em}

\renewcommand{\chaptername}{\normalsize{CHAPTER} }  %%% Capitalizes the word chapter in chapter heading   %%%

\setcounter{secnumdepth}{5} 	%%% Sets number of subsections you may have %%%

\titlespacing{\chapter}{0pt}{.125in}{.125in}  	%%% Controls spacing above and below chapter heading %%%
\titlespacing*{\section}{0pt}{.5em}{.5em}     %%% Controls spacing above and below section heading %%%
\titlespacing*{\subsection}{0pt}{.5em}{.5em}  	%%% Controls spacing above and below subsection heading %%%
\titlespacing*{\subsubsection}{0pt}{.5em}{.5em}    	%%% Controls spacing above and below subsubsubsection %%%
\titlespacing*{\paragraph}{0pt}{.5em}{.5em}     		%%% Controls spacing above and below paragraph heading %%%

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%CHANGES MADE TO TABLE OF CONTENTS, LIST OF TABLES, LIST OF FIGURES  

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

\counterwithout{table}{chapter} 		%%% Numbers tables continuously, i.e., 1,2,3,... %%%
\counterwithout{figure}{chapter} 		%%%Number figures continuously, i.e., 1,2,3,... %%%
\setcounter{tocdepth}{5}  			 %%% Allows number of sections to show in table of contents %%%
\setlength\cftaftertoctitleskip{-2pt} 		%%% Reduces amount of space between title table of contents from below %%%
\setlength{\cftbeforetoctitleskip}{-2em}   %%% Reduces amount of space between title table of contents from above %%%

\setlength{\cftbeforeloftitleskip}{-2em}   	%%%  Removes white space before List of Figures title %%%
\setlength{\cftafterloftitleskip}{-2em}   	%%% Removes white space after List of Figures title %%%
\setlength{\cftbeforelottitleskip}{-2em}   		%%%%% Removes white space before List of Tables title %%%%%%%
\setlength{\cftafterlottitleskip}{-2em}  		%%% Removes white space after List of Tables title %%%
\setlength{\cftbeforechapskip}{.1ex}  		 %%%%%% Reduces white space above chapters in table of contents  %%%%

\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}  	%%% Puts dotted lines in front of chapters in table of contents %%%
\renewcommand*\contentsname{\hfill \normalsize{\textnormal{Table of Contents}} \hfill} 	%%% Puts title of table of contents in center of page %%%
\renewcommand\cftchappagefont{\mdseries}   		%%% Makes chapter page numbers non-bold text %%%
\renewcommand\listfigurename{\hfill \normalsize{\textnormal{List of Figures}} \hfill}	 %%% Centers LOF title and changes font style %%%
\renewcommand\listtablename{\hfill \normalsize{\textnormal{List of Tables}} \hfill}   		%%% Centers LOT title and changes font style %%%
\renewcommand{\cftfigpresnum}{Figure\ } 		%%% puts "Figure" in front of number on list of figures page %%%
\renewcommand{\cfttabpresnum}{Table\ } 		%%% puts "Table" in front of number on list of tables page %%%
%\renewcommand{\cftchapfont}{CHAPTER }
%\renewcommand{\cftchappresnum}{\normalfont{CHAPTER} }  %%% Puts content in front of number in table of contents %%%
\renewcommand{\cftchappresnum}{\normalfont{CHAPTER} }   		%%% puts the word "CHAPTER" before chapter number in table of contents %%%
\renewcommand{\cftchapnumwidth}{1in}  		%%% Changes chapter number width box %%%
\renewcommand\cftchapaftersnum{.}		% adds dot after chapter number in table of contents


%%% Spaces out the words "Figure" and "Table" in the List of Figures/Tables %%%
\newlength{\mylenf}
\settowidth{\mylenf}{\cftfigpresnum}
\setlength{\cftfignumwidth}{\dimexpr\mylenf+1.5em}
\setlength{\cfttabnumwidth}{\dimexpr\mylenf+1.5em}

\makeatletter
\newcommand\listoftablesandfigures{%
    \chapter*{List of Tables and Figures}%
    \phantomsection
\@starttoc{lof}%
\bigskip
\@starttoc{lot}}
\makeatother

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%   CHANGES MADE TO BIBLIOGRAPHY 

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

\renewcommand{\bibname}{\normalfont{References}}          %%% Changes name of Bibliography page to "References" %%%
\usepackage{biblatex}
\addbibresource{olneyMS2019.bib}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

 %CHANGES MADE TO PAGE LAYOUT 
 
 %$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
 %\tabulinesep=1.3mm %Gives extra space to line spacing
 %\extrarowsep=1mm %Gives extra space to height of columns
 \let\cleardoublepage\clearpage 	%Removes blank pages between chapters
\linespread{1.6}
\setlength\parindent{.5in}  		%%% Sets indentation depth %%%
\setlength\parskip{0pt}   		%%% removes space between paragraphs %%%
\fancyhf{} 				%%% clear all header and footer fields %%%
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}  		%%% Removes line in header %%%
\renewcommand{\footrulewidth}{0pt}  	%%% Removes line in footer %%%

%Keeps long lines in paragraph from spilling into margins.
\tolerance=10000
\pretolerance=1000

%%%         Redefining plain style which is automatically applied to chapters (including Bibliography)        %%%
\fancypagestyle{plain}{%
\fancyhf{} 				%%% clear all header and footer fields %%%
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}
\pagestyle{fancy}

\rhead{\thepage} 		%%% Put page number in top right hand corner %%%
\cfoot{} 			%%% Removes numbers in center footer %%%
\lhead{}		 %%% Removes left header %%%
\pagenumbering{roman}  %%% Specifies type of page numbering %%%

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%CHANGES MADE TO CAPTIONS, TABLES, FIGURES

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\newcolumntype{Y}{>{\centering\arraybackslash}X} %creation of new column type that centers entries

% spacing for tables
\setlength{\abovecaptionskip}{3ex}
\setlength{\belowcaptionskip}{-3ex}
\setlength{\textfloatsep}{2.5ex}
\setlength{\intextsep}{2.5ex}

\begin{document}
\doublespacing  	%%% Sets double spacing for document %%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%          								TITLE PAGE!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\begin{center}
Reward Design for Reinforcement Learning Applied to Low-Level Control of Quadrotor UAV*\\   		%title of thesis or dissertation
Evan J. Olney \\  	 %Name
North Carolina A\&T State University  	%School name
\vspace{110.4pt}
\end{center}
\begin{center}
A thesis proposal submitted to the graduate faculty \\ 
in partial fulfillment of the requirements for the degree of \\ 
MASTER OF SCIENCE\\
Department: Electrical and Computer Engineering \\      %Department name
Major: Electrical Engineering \\           %Major of study
Major Professor: Dr. Abdollah Homaifar \\    %Major professor
Greensboro, North Carolina \\   %City, State
2019    %year
\end{center}
\vspace{3ex}

\newpage%%%%% Command starts a new page  %%%%%%%%%
\singlespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%          							  SIGNATURE PAGE!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{page}{2}
\begin{center}
The Graduate School\par North Carolina Agricultural and Technical State University\par This is to certify that
the Master's Thesis Proposal of
\end{center}
\vspace{.25in}

\begin{center}
Evan J. Olney  	 %Name Here
\end{center}

\vspace{.25in}
\begin{center}
has met the thesis requirements of\par North Carolina Agricultural and Technical State University
\end{center}

\begin{center}
Greensboro, North Carolina\par 2019 	%City, State, Year
\end{center}

\begin{center}
Approved by:      	%List of individuals who must sign off on your thesis/dissertation
\end{center}
\vspace{1.5ex}
\begin{center}
\noindent\begin{tabular}{ll}  			%%% Marks beginning of tabular argument %%%
\makebox[2.5in]{\hrulefill} \hspace{1in} & \makebox[2.5in]{\hrulefill}\\
Dr. Abdollah Homaifar & Dr. Ali Karimoddini \\
Major Professor & Committee Member\\[4ex] 		%%% Adds space between the two sets of signatures %%%
\makebox[2.5in]{\hrulefill} \hspace{1in} & \makebox[2.5in]{\hrulefill}\\
Dr. Steven Xiaochun Jiang & Dr. Berat Erol\\
Committee Member & Committee Member\\[4ex]
\makebox[2.5in]{\hrulefill} \hspace{1in}\\
Dr. Abdullah Eroglu\\
Department Chair
\end{tabular}		 %%% Ends tabular argument %%%
\newline
\end{center}
\vspace{4ex} %spaces out lines for signatures
\begin{center}
\begin{tabular}{l}
\makebox[2.5in]{\hrulefill}\\		%%% Signature for Dean of Graduate School %%%
Dr. Sanjiv Sarin\\
Dean, The Graduate School
\end{tabular}
\end{center}
\newpage
\thispagestyle{fancy}
\rhead{\thepage}
\cfoot{}
\doublespacing  	%%% Sets spacing to double %%%
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%								         COPYRIGHT PAGE!!!     
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% Here, we put text in the bottom corner of the page using the following sequence of commands %%%
\null
\vfill
\begin{flushright} 
\copyright \hspace{1ex}Copyright by\\
Evan J. Olney\\
2019
\end{flushright}
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%         							  BIOGRAPHICAL SKETCH!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{\hfill \normalfont{Biographical Sketch} \hfill} 
\textbf{This page is required and should appear in paragraph form. Your name should appear here exactly as it appears on your title page. Consult the manual for additional details.} 
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%										DEDICATION!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{\hfill \normalfont{Dedication} \hfill}
\textbf{This page is optional and appears after your biographical sketch.} 
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									 ACKNOWLEDGEMENTS!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{\hfill \normalfont{Acknowledgements} \hfill}
\textbf{The acknowledgement page is also optional. The acknowledgements should be in paragraph form.} 
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									TABLE OF CONTENTS!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents 	%%% Sets page for table of contents %%%
\thispagestyle{fancy}   	%%% Changes pagestyle of single page %%%
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									      LIST OF FIGURES!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\listoffigures    		%%% Call list of figures %%%
\addcontentsline{toc}{chapter}{\normalfont{List of Figures}} 	 %Adds List of Figures to Table of Contents
\thispagestyle{fancy}   	%Sets pagestyle for specfic page
\newpage  			%%% Starts a new page %%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									LIST OF TABLES!!!   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\listoftables  	 %%% Call list of tables %%%
\addcontentsline{toc}{chapter}{\normalfont{List of Tables}}  	%%% Adds List of Tables to Table of Contents %%%
\thispagestyle{fancy}
\newpage 		%%% Starts new page %%%
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									 ABSTRACT!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{page}{1}   	%%% Changes starting page number %%%
\section*{\hfill \normalfont{Abstract} \hfill}
\noindent The past few decades have produced many successful applications of machine learning. In the area of automated controls, machine learning controllers have been developed to take advantage of the tendency for computers to stumble upon avenues to solutions people either overlook or simply do not have the computational capability to explore. This project explores the use of model-free reinforcement learning to apply to controls problems. The components of reinforcement learning are defined in terms of components involved with automated controls to produce a controller. By focusing on lower level control, the learning algorithm is expected to have the ability to control for specific performance metrics similar to those of a control problem. In several case studies, reinforcement learning is used to produce controllers by designing the learning problem with their respective control parameters. The final case study features a quadrotor flight controller capable of producing a target rotation velocity with a comparable performance to the traditional PID controller design. This low-level motor control is the basis for the stability of quadrotor flight. By shaping the reward function to resemble the control metrics, the resulting controller can be indirectly designed to give priority to different control tasks.
\thispagestyle{fancy}
\addcontentsline{toc}{chapter}{\normalfont{Abstract}} 	 %%% Adds Abstract Table of Contents %%%
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									 Introduction!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Introduction}]{Introduction} 		%%% \chapter[appearance in table of contents]{appearance in body} %%%

% Automating Tasks\par
% Artificial Intelligence \par
Recent advances in artificial intelligence has created both excitement and concern in the mainstream of modern society. The ability for a computer to learn and make decisions on its own could have the capacity to reduce manual effort on tedious tasks, and make discoveries that have been overlooked or are out of reach for human capacities. One of the fears is having a loss of control because of the intelligence's unpredictability. Although the concerns are not without merit, the benefits of having another source of intellect, advances in computational technology, and the promise of commercial viability, has propelled research into advancing AI. \par
% Machine Learning \par
With every new hardware and software product claiming to contain "advanced AI," we are led to believe that tech companies have cracked the code to programming human intellect. Most of these innovations, however, are using techniques from a subset of artificial intelligence called machine learning. There are many definitions for machine learning, most explaining that it is the use of algorithms to give a program the ability to make updated decisions based on experience. These algorithms often fall into two main classifications: Supervised Learning and Unsupervised learning. A third classification of learning algorithms is typically separated from the other two, but is often argued as a subset of one or a mixture of both: Reinforcement learning. \par 
% Supervised Learning \par 
An algorithm that learns with the assistance of the correct answers at hand is called a supervised learning algorithm. The most commonly used form of machine learning, supervised learning is applied to tasks where, given set of feature data, the output generated is determined as correct or incorrect in context to its specified task. This determined judgment is based on a pre-obtained data set labeling samples of the feature data with correct instances. The algorithm uses this data to learn how to produce correct outputs from the set of features. For example, a data set of flight dynamics and aircraft operation states may be used by a supervised learning algorithm to predict the state an aircraft is operating based on given flight dynamic samples, i.e. feature data. By finding the unique spaces of input features associated with the operation state, the algorithm can continue to learn with every example given to it. One limitation of these algorithms is that they typically cannot learn beyond the labeled samples they have been shown, e.g. if the algorithm has only been given examples of the aircraft in take-off and cruising states, then the resulting learned model can only classify an aircraft that is in a landing state as either of these two operation states, or neither. The other limitation is that supervised learning algorithms must be provided labeled data, which may be difficult or costly to obtain. \par 
% Unsupervised Learning \par 
While supervised learning is given the answers that the learner should be looking for, unsupervised learners are just given the data and asked what answers could make sense. With only the input data, unsupervised learning algorithms use a variety of techniques to produce boundaries of where possible classes may be determined. By using similarity metrics, like density and distance, an unsupervised learning algorithm can learn how to organize data without predetermined labels. These algorithms are often used for finding patterns in complex data sets, e.g. a data set of aircraft flight test dynamics can be submitted to an unsupervised learning algorithm to find probable data spaces that determine different operation states without being given any named operations, like landing or cruising. The resulting groups of operation states can then be analysed, named, and used to determine the operation state of the aircraft during future flights. Making determinations without a defined goal may provide unexpected relationships within the data which describe useful information regarding the nature of the subject matter. Many of these relationships may be defined prematurely, too broad for utilization, or not useful in the context of the data. \par 

\section{Reinforcement Learning}
Instead of attempting to make any and all connections, the learning algorithm can be guided toward specific learning goals. Reinforcement learning (RL) is a machine learning technique inspired by the psychological theory of the same name. Although slightly different in their respective fields, they are both based on how using a factor of reward affects the learning process. In machine learning, reinforcement is using a reward signal to guide an agent to achieve a desired goal without explicitly labeling correct and incorrect behavior. By providing a signal in reference to the desired behavior, RL can benefit from the specificity associated with supervised learning algorithms. With the omission of labeling the learning agent's determinations, RL can also explore the input-output relationship space with a similar generality of unsupervised learning. Although the premise of learning from experience is a fundamental principle for how we believe a person or animal may learn, translating this principle to a computational design has not taken a universal form. There are many branching methods for applying this learning style \cite{sutton2018reinforcement}. \par
 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
 \centering
 \includegraphics[scale=.5]{images/rl_struct.png}
  \caption{Basic reinforcement learning structure}
  \end{figure}
The general structure of RL includes an agent and an environment for which the agent can interact with and perceive. When the agent acts on its environment, it may then detect any changes to this environment including any reward, \(r\), as a result. By choosing actions, \(a\in A\), based on these perceptions, referred to as states, \(s\in S\), the agent is trying to maximize its reward during its time of functioning. The environment is typically assumed as Markovian, meaning the probability for every state transfer, \(p(s'|s,a)\), is solely dependent on the current action at the current state. Although there may have been a chain of actions to arrive at the current state, the previous actions are assumed to not be a factor in the next possible state transfer as shown by a simplified version of Markov's principle for this context: $p(s_{t+1}|s_0,a_0,s_1,a_1,...,s_t,a_t) = p(s_{t+1}|s_t,a_t)$. This reduces the scale of the learning problem by only focusing on choosing actions as they pertain to the immediate state instead of also accounting for any or all previous actions simultaneously.\par 

%\begin{equation}
%    p(s_{t+1}|s_0,a_0,s_1,a_1,...,s_t,a_t) = p(s_{t+1}|s_t,a_t)
%\label{eq:markov_principle}
%\end{equation}

To apply reinforcement learning to a problem there are many factors to consider, including defining the RL elements and designing the learning algorithm. The learning agent must have an ability to observe the environment in order to assess its state, and an ability to perform an action that may change this state. Actions are limited to what an agent can do. These actions must be defined in the boundary of what an agent can attempt in all states of the environment. The form of the state is the chosen observation space from the available information. Although a large amount of information may be known about an environment, not all of this information may be necessary to obtain a solution to the problem. Feature selection and extraction methods for machine learning \cite{khalid2014survey} and even specifically for reinforcement learning \cite{liu2015feature} have been studied for the purpose of choosing the best features to monitor for the learning process. These features are used as states for the agent to observe and choose following actions. \par

\subsection{Rewards}
The reward signal is the value that the agent must maximize by learning to make decisions that favor its increase. Rewards are applied at state transitions, $p(s',r|s,a)$. This signal is normally a scalar value that rewards or penalizes the agent after a state transfer. The condition for receiving a reward or penalty are a combination of the goal and observable states of the environment. A reward may be a result of reaching a goal or a measure of getting closer to one. To give an example, if a mobile robot has the ability to sense what room it is in and the goal is for it to learn a method to travel to a goal room, the reward signal may be a positive value that is triggered when the robot has reached the goal room, a negative value while it is not in the goal room, or a combination of the two. How exactly these different choices will affect the outcome of the learning process are not certain considering the accumulation of reward can be designed in an infinite number of ways.

\subsection{Policy Structure}
A policy, \(\pi\), is used to map the actions for the agent to take based on the perceived state of the environment. These policies are meant to steer the agent toward the path that produces the maximum level of reward by giving the action commands with the highest probability of resulting in state transfers that produce, or will produce, the maximum possible reward. These guides are represented as anything from a function of the state variables to a look-up table of the state with the action as the value. The structure of the policy is important for the computational cost of the learning process and the available learning methods. The format of the agent and environment can be instrumental in choosing a structure. For example, if the agent has a static finite set of actions, \(A\), that may be attempted throughout the environment, and the environment state can also be measured in a finite low dimension discrete space, \(S\), then a table policy or coded string may easily represent a relationship of state to action, \(\pi (s)\). A table policy takes the current state as an index with the value of the table's cell at that index as the action, $a_t = \pi (s_t)$. This is a deterministic representation of the policy, but considering the stochastic nature of the environment, a policy may also be better suited as a stochastic representation, $\pi(a|s)$. \par

\subsection{Policy Optimization}
If the goal is to find the optimal policy that yields the maximum rewards in a given amount of operational time, an obvious solution is to try every possible action in every possible sequence in every possible state enough times to have a significant amount of data to conclude the state transfer probabilities between all states and actions. An analysis of the results is almost guaranteed to yield a path to maximum reward. The scale of most problems make this solution computationally impractical. Instead, techniques have been created in order to arrive at an appropriate solution without the need to try every possibility.\par
\begin{equation}
    v_{\pi}(s)=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1}|S_t=s], \text{ for all } s \in S
\label{eq:val_func}
\end{equation}
Value functions, like equation \ref{eq:val_func}, gives a value of the agent being in the given state based on the expect sum of rewards if the policy is strictly followed for its lifespan. Many techniques use this equation as a metric for comparing different policies. Other techniques adapt this function in a way to update parts of its policy to further increase its value. These branching techniques are Policy Search methods, e.g. CMA\cite{hansen2001completely} or DPG\cite{silver2014deterministic}, and Policy Improvement methods, e.g. Q-Learning\cite{watkins1992q} or SARSA \cite{sutton2018reinforcement}. The former involves primarily with discovering a completely new policy while the latter primarily uses the value estimations to assess and improve parts of the policy toward a goal. Although often categorized in this fashion, many modern usages of the techniques fall somewhere between a policy search and iterative improvement, e.g. DDPG\cite{lillicrap2015continuous}, Deep-Q\cite{mnih2015human}, or TRPO\cite{schulman2015trust}.\par

%%%%%%%%
% MOTIVATION
%%%%%%%%

\section{Applications of RL}
%Using supervised learning\par 
RL has been applied to a multitude of problems in a wide range of fields including biology \cite{neftci2002reinforcement}, natural language processing \cite{li2016deep}, and computation optimization \cite{rao2009vconf}, to name a few. Most applications attempt to optimize decisions in highly stochastic environments that are difficult to model or constantly change, like market values \cite{nevmyvaka2006reinforcement} or weather \cite{kuremoto2007forecasting}. With the intuitive nature of trail and error, RL can be applied to most problems where the environment is available to be acted upon by a learning agent.

\subsection{Games}
Games are a commonly used platform for testing algorithms because they have clear goals, contain a sense of complexity for accomplishing said goals, and are relatively cheap for a machine to operate. Although they may have easily measurable metrics of play, their space of possibility can be exponentially large. For example, the game of go consists of placing a stone on an $n \times n$ grid turn by turn in order to obtain empty space and surround opponent stones. At the time of this writing, it is impossible for the strongest publicly known super computer to brute force all possible moves for any given board configuration in order to guarantee a win in any game. From this challenge, a research team created a system that learned how to defeat some of the best human players in this game using reinforcement learning by playing games against itself to develop winning strategies \cite{silver2016mastering}. This feat demonstrated the ability for well designed reinforcement learning algorithms to excel human performance at a specific game without directly using the single greatest advantage computers normally provide, rapidly searching entire spaces. \par 
Although the system designed to tackle the game of Go performs well, it cannot play any other games. The learning algorithm, however, has been used in clever ways to attempt general learning in video games \cite{mnih2015human}. By using reinforcement learning to take visual input from a video game, a team at DeepMind were able to use a single learning algorithm to play a variety of simple video games successfully. The success of this work has fueled more research into reinforcement learning algorithms using games as testing platforms. They continue to not only exercise the algorithms' ability to play games \cite{andersen2018deep}, but reinforcement learning is also utilized to aid in developing better games as well \cite{andrade2005extending}. Games require mechanics to make sure that the play is fair and challenging. By using RL in the game development, game play can become balanced and adaptive. \par 

\subsection{Neural Machine Translation}
Spoken language is far different from the language of mathematics and programming. The rules in natural language communication are complex and do not transfer well between all languages, making translation a very difficult task for machines to perform. The field of natural language processing (NLP) contains numerous techniques to capture the nuances in natural language. One branch of techniques, aptly named neural machine translation, involve neural networks to predict sequences of words. Reinforcement learning may be applied to the learning process to improve the translation results of this technique \cite{wu2018study}. \par 

\subsection{Robotics}
Reinforcement learning has been applied to robotics for decades. From industrial production robots \cite{fitzgerald2013developing}, to unmanned vehicles, to attempting control challenges like ball-in-a-cup \cite{kober2013reinforcement}, RL continues to find use in robotic platforms by exploring methods for autonomous physical motion and manipulation. Controlling robots and unmanned vehicles becomes complicated through the layers and related degrees of motion and available control signals. By using RL, specific behaviors are learned without directly modeling. This is an attractive quality for finding applications of general purpose robots and vehicles without having to remodel the controlling aspects of the system for every desired behavior. As an example, a robot arm can be trained to pick up objects, but also learn how to press buttons, all without remodeling the desired behavior, but instead updating the goal functions of the learning algorithm.\par 

\subsubsection{Controls}
Control theory involves the study and application of how to obtain desired outputs from a system under specified constraints. By studying the system and assessing its parameters, a controller may be designed to drive a system to produce a desired output based on its inputs and internal behaviors. The main elements of control are the inputs, outputs, and controllable elements. A behavior model is a mathematical model of how measurable disturbances affect the output of the system. This model does not need to be a perfect representation of how each component of the system functions, but must produce similar outputs from similar inputs compared to the real world system.\par
% Control Scheme Figure
\begin{figure}[H]
    \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
    \centering
    \includegraphics[scale=.5]{images/feedback.png}
    \caption{Basic Feedback Control Scheme}
    \label{fig:feedback}
\end{figure}
Many real-world systems are highly complex and cannot be easily modeled because some of the elements are not easily measured or have unknown relationships to other variables in the system. Every variable that is important for the behavior model to resemble the system should be accounted for in the model and adds to the dimension of the system. How each element in the system interacts with each other and from other disturbances makes modeling a system for control an intense study of observation and mathematics for the designer of a controller. \par


%%%%%%%%
% PROBLEM STATEMENT
%%%%%%%%


\section{Challenges of RL Application}
%Sparse Rewards\par 
A learning agent that is given no information must still choose an action while it exists in the environment. It will typically randomly act until it has found a difference in received reward which can be used to update its perspective on increasing actions. Even with advanced exploration strategies, some rewards are still too complicated to randomly stumble upon. This is known as the sparse reward problem and contributes to high sample inefficiency. By having a sparse reward, the agent may find solutions that are not intuitive to human designers. However, when reward signals are designed to be at every step imaginable, the agent is limited to learning the intended solution, which may not be optimal. Even when such solution is the optimal solution, the reward design must then be redesigned for all different environments, even if they are similar. The best solution would be to find a way to generally update the agent's feedback while minimizing the engineering of case-specific reward signals.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%									 Literature

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter[\normalfont{Approaches}]{Approaches} 

\section{RL Applied to Controls Tasks}
Over the years, RL has been applied to various controls tasks, often in the field of robotics and unmanned vehicles\cite{kober2013reinforcement}. Unmanned vehicles often have several levels of control, categorized from the higher level, e.g. path planning and position control, to the lower levels, e.g. motor and signal control. Application of RL has spanned all of these levels and have shown success in producing policies that aid with both choosing parameters for existing control techniques \cite{bou2010controller} to choosing the actions as the controller itself \cite{hwangbo2017control}.\par

\subsection{Flight Reference for System Identification}
Autonomous flight control of a helicopter to perform aerobatic maneuvers using reinforcement learning was performed to demonstrate the ability of reinforcement learning to learn in stochastic non-linear environments \cite{ng2006autonomous}. For this project, supervised learning was used to identify a model of the helicopter from data gathered by a professional pilot performing a specific flight maneuver. This model is then used to construct a simulator that can take the helicopter control inputs and produce results based on the model. For learning a flight controller capable of performing this maneuver, a policy search method was used to maximize an expected reward function. After training the controller to learn how to perform this flight maneuver in the simulation, the policy was transferred to the hardware used in gathering the initial data. To test the ability of this controller, the pilot flew the helicopter into position, then switched the control to the controller. \par
The policy for this experiment was a neural network that was trained on 5 parameters of each decoupled PD controllers; one pitch controller and one roll controller. The weights included $x, v_x, \text{pitch}, \delta_{lat}$ for the pitch controller, and $y, v_y, \text{roll}, \delta_{lon}$ for the roll controller. Each network included a hidden layer between the delta and position values to include non-linearity for higher adaptation in the distance shifts.
% Network diagram
\begin{figure}[H]
    \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
    \centering
    \includegraphics[scale=.6]{images/heli_poli.png}
    \caption{Pitch (left) and Roll (right) Network Diagrams}
    \label{fig:heli_poli}
\end{figure}
\begin{equation}
    x^2+y^2+v_x^2+v_y^2+.0001\delta_{lat}^2+.0001\delta_{lon}^2
\label{eq:cost_quad}
\end{equation}
Several costs functions were used for the purpose of minimization based on simulation outputs. Using a cost function is similar to giving a negative reward, i.e. penalizing the agent. Equation \ref{eq:cost_quad} shows a general cost function that was considered along with a few others of the quadratic type. Equation \ref{eq:cost_lin_dist} is a part of the cost function aimed at keeping the distance within a reasonable location space. As a final addition to the cost function, a large penalty was given when a variable exceeded the space of the original observed data. 
\begin{equation}
    10 \frac{x^2}{x^2+1}+10\frac{y^2}{y^2+1}
\label{eq:cost_lin_dist}
\end{equation}
This early demonstration proved the ability of reinforcement learning to learn how to fly an unmanned aerial vehicle in a limited scope. The controller was trained on a simulation to avoid mistakes made while training that would crash the helicopter, but also creates a more expedient training process. Although this maneuver is not the only task possible to train against, to fly another maneuver with this technique another system identification task and simulation must be built. The learning parameters, like the reinforcement signal, will also have to be updated to produce an accurate control. It is unreasonable to create new simulations for every aerial maneuver for a completely autonomous vehicle. 

\subsection{Model-Based Outer-Loop Reinforcement Learning}
%Learning Flow diagram
\begin{figure}[H]
    \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
    \centering
    \includegraphics[scale=.6]{images/control_learning.png}
    \caption{Outer Loop Controls RL Diagram}
    \label{fig:control_learning}
\end{figure}
Reinforcement learning is often applied to control or partially control a quadcopter UAV because of the challenge of non-linear flight control dynamics, but a simple mechanical construction. Most application involve a mathematical model of the flight dynamics of the UAV and us RL to learn the parameters associated with the modeled control, \cite{bou2010controller} \cite{santos2012experimental} \cite{lupashin2010simple}. Actions in these cases are typically parameter value selection. When the policy chooses an action, these parameters are set in the controller where it is simulated. The results are passed through a cost function that can then be used to generate a reward signal. This reward is processed by the learning algorithm to update the policies choice dynamic. In these cases the environment is only the flight model, an ideal environment, for the RL agent to learn values of the control parameters. Results from ideal environments can be uncharacteristic of the real world, requiring there to be modifications made when applying hardware.\par





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 %									 Methodology
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Reward Design for Control}]{Reward Design for Control}
There are similarities with general controls problems and reinforcement learning problems that may be utilized to establish a strategy for applying reinforcement learning to controls. Although these similar features are not directly transferable in every case, they reduce some of engineering effort needed to make some of the definitions. This research explores a structure composition of making these reinforcement learning parameter definitions that may be applied to low-level controls problems. \par

\section{Defining RL Elements}
Controls tasks consists of a target system for prospective control, often referred to as the plant. The concerns that are necessary for designing the controller are limited to this system's inputs, outputs, and behavior. External disturbances may be modeled as a form of input into the system. This focus makes it simple to define the environment as the target system. The agent, however, is described as the learner attempting to accomplish a task by performing actions and learning from the environment. In a feedback control structure, the controller takes the output of the system and performs a calculation to adjust the input of the system to match with the reference of desired output. This controller behavior is much similar to that of a learning agent, making it the obvious choice for its definition. \par 

\subsection{Actions}
Each problem relies on the ability to control some inputs to the system. If a system is uncontrollable, a controller is impossible to design, and if an agent's actions have no effect on the environment, then the best policy will be one of pure random actions. In controls theory there are input variables that can be manipulated, but whether this manipulation will have the ability to push the target system variables is not immediately certain, so it must be assessed based on the generated model. Without a specific metric to guarantee controllability of the system, the problem must assume that the system is indeed controllable. These manipulations of the input variables will be defined as the actions, $a \in A$. The agent will be learning in the environment and generate samples of outcome pairs from the observations based on these chosen actions.\par

\subsection{States}
The agent must be able to measure the state, $s_t$, at every step and choose an action accordingly. Environment states are defined to give the agent sufficient information to perform the desired task. Variables that can be measured in the system may also be used to define the state. Low-level control tasks may contain discreet or continuous states as a part of its observations. 

\section{Defining Reward Function}
When controlling a system, there are several common concerns with respect to the performance of the controller. For each of these concerns, a reward element is defined. The reward elements are scaled and combined to create a reward function that is designed to push the controller to worry about all of the common concerns simultaneously.


\section{Policy Optimization}
Most low-level controls tasks operate using continuous values, and therefor should choose optimization methods designed to handle continuous states and actions. For example, proximal policy optimization (PPO) is a neural network policy optimization technique that has combined elements of other policy optimization techniques to produce a strong efficient optimization algorithm for deep reinforcement learning tasks. 

%Advantage Function
\begin{equation}
    \hat{A}_t=\delta_t+(\gamma\lambda)\delta_{t+1}+\ldots+(\gamma\lambda)^{T-t+1}\delta_{T-1}
\label{eq:advantage}
\end{equation}
%Advantage delta
\begin{equation}
    \delta_t=r_t+\gamma V(s_{t+1})-V(s_t)
\label{eq:advantage_delta}
\end{equation}
%loss Function
\begin{equation}
    L^{CLIP}(\theta)=\hat{\mathbb{E}}_t[min(r_t(\theta)\hat{A}_t,\textnormal{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)]
\label{eq:ppo_loss}
\end{equation}

\begin{algorithm}[H]
\SetAlgoLined
\For{iteration=1,2,... \do }{
    \For{actor=1,2,...,N \do }{
    Run policy \(\pi_{\theta_{old}}\) in environment for \(T\) timesteps \;
    Compute advantage estimates \(\hat A_1,..., \hat A_T \)\;
    }
    Optimize surrogate \(L\) wrt \(\theta\), with \(K\) epochs and minibatch size \(M<=NT\)\;
    \(\theta_{old} \leftarrow \theta\)\;
}
\caption{PPO, Actor-Critic Style \cite{schulman2017proximal}}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 %									 Experiment Design
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Experiment Platform}]{Experiment Platform}

\section{Gym}
OpenAI's Gym is a test bench software for open source research into reinforcement learning algorithms \cite{brockman2016openai}. Gym environments have a built-in set of actions, observable states, rewards, termination signal, and additional information regarding the environment. By enclosing this environment into its own object, an external algorithm may submit action choices to the environment and read its resulting state and reward. Reinforcement learning algorithms are designed to use this information to inform its policy how to update in order to get the most reward from the environment, making this structure convenient for focusing on the algorithms instead of the environment representation. 

\section{Baselines}
\subsection{Proximal Policy Optimization}
\subsubsection{Time Analysis}

\section{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 %									 Case Study 1
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Continuous Mountain Car Problem}]{Continuous Mountain Car Problem}
The mountain car problem has been used for several decades as a test bench for reinforcement learning algorithms \cite{singh1996reinforcement}. Originally, the problem involved a car that is at rest in a valley without the acceleration power necessary to pull itself directly up the hill in front of it. With a hill behind it, and the ability to accelerate in either direction, the car must drive forward and reverse consecutively until it builds enough momentum to make it to the top of the mountain before it. Intuitively, a person would know how to do this, but the challenge is for a computer to learn a way of performing this task with only its perception of position and velocity.\par
The action space for the original problem is simply full force backwards, full force forward, or no force exhorted. This has been modified to being a continuous action space with the car having the ability to apply force between -100\% and 100\%, representing completely backward and forward forces respectively. The range of positions is between -1.2 and 0.6. The maximum speed of the vehicle is 0.07, and its starting position is set randomly between -0.6 and -0.4. 

 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
 \centering
 \includegraphics[scale=1]{images/mountaincar.png}
  \caption{Mountain Car Initial Position}
  \label{fig:mountaincar}
  \end{figure}

As a benchmark problem, this challenge is designed to test the learning algorithm's ability to explore the agent's operating ability by providing a penalty for every step that the agent is not at the goal. A simple and common design for this reward penalty is by rewarding -1 for every time step. An agent is then expected to learn that the best way to maximize its reward, i.e. minimize its penalties, is to reach the goal position with the least amount of steps. With a reward that is not directly related to the subtle difference between every few timesteps, the algorithm must have a way to explore the environment to reach a good outcome that isn't immediately gratified. Another form of this is having no reward until the agent reaches the goal.

\section{Reward Experiment}
To test experimentally the efficacy of a general reward function to handle the individual concerns of a controls problem simultaneously, an experiment was designed testing the elements and their combinations.  \par 

\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
    Reward Element & Function & Coefficient\\
    \hline\hline
    Goal & $\left \{ \begin{tabular}{cc} $r_g = 0$ & $S \neq S_g$ \\ $r_g = 1$ & $S_t = S_g$\end{tabular} \right \}$ & $c_g = 100$\\
    \hline
    Error & $r_r=-(S_g-S_t)$ & $c_r = 0.061$\\
    \hline
    Speed & $r_s=-1$ & $c_s = 0.1$\\
    \hline
    Energy & $r_e=-A_t^2$ & $c_e = 0.1$\\
    \hline\hline
    Velocity & $r_v=V^2$ & $c_v = 20.5$\\
    \hline
\end{tabular}
\caption{Reward elements of control concerns}
\label{table:elements}
\end{table}


\begin{equation}
    r_t=c_g r_g + c_r r_r + c_s r_s + c_e r_e
\label{eq:rtelements}
\end{equation}

The elements represented in table \ref{table:elements} are all reflected in the reward function in equation \ref{eq:rtelements} except the velocity element that was initially designed to directly give a reward function to the apparent solution to the problem. Before applying this reward function directly, reward elements were tested individually and with some combinations. The coefficients for each reward served to both give control over which reward element to focus on as well as to scale the rewards within a range of each other. Having a maximum episode length, it is possible to gauge the maximum penalties for each element given their ranges. The coefficients in the table were chosen to give a maximum value of each reward element to 0.1 except for the goal reward, which could only be given once per episode. If the agent did not reach the goal by timestep 999, then the episode will end.\par

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
    Reward Element(s) & Definition\\
    \hline\hline
    $r_e$ & Energy\\
    \hline
    $r_r$ & Error\\
    \hline
    $r_g$ & Goal\\
    \hline
    $r_s$ & Speed\\
    \hline
    $r_v$ & Velocity\\
    \hline
    $r_e r_g$ & Energy with Goal\\
    \hline
    $r_r r_g$ & Error with Goal\\
    \hline
    $r_s r_g$ & Speed with Goal\\
    \hline
    $r_v r_g$ & Velocity with Goal\\
    \hline
    $r_a$ & All elements\\
    \hline
\end{tabular}
\caption{Reward element experiments}
\label{table:reward_experiments}
\end{table}

\subsection{Evaluation}
10 controllers were trained with each reward element and combination by using PPO with identical hyperparameters. Every controller was then run in the environment 40 times while recording several external metrics from the reward value itself. These metrics included the average speed, energy, training time, and success rate. \par
The controller's speed refers to the average number of timesteps the controller needed to reach the goal. The energy is a measurement of how much total force is exhorted throughout the episode, left graph in figure \ref{fig:mountaincar_energy}, or the amount of energy used per timestep, right graph in figure \ref{fig:mountaincar_energy}. A controller was considered successful if it could reach the goal before 990 timesteps in all 40 environment runs. This metric sheds some light on the learning stability of training the controller with the specified reward combination. \par

\section{Results}
The red bar graphs show metrics that should be minimized while green graphs should be maximized. When looking at the speed of the controllers in figure \ref{fig:mountaincar_speed}, the $r_s$ reward element produces the controllers that find the quickest route to the goal. This is followed by the full reward function, $r_a$. The energy, goal, and velocity elements show the slowest controllers to reach the goal.\par
 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
 \centering
 \includegraphics[scale=.7]{images/0speed.png}
  \caption{Average Controller Speed}
  \label{fig:mountaincar_speed}
  \end{figure}
  
Controllers created with the energy elements, $r_e$, $r_e r_g$, and $r_s$, far out perform the other elements and combinations in the realm of energy consumption during the episodes. Although all 3 elements use the least energy per episode, only the singular reward element controllers have an outstanding performance of reducing the energy per step as seen in figure \ref{fig:mountaincar_energy}.\par 

 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		
 \centering
 \includegraphics[scale=.5]{images/0energy.png}
 \includegraphics[scale=.5]{images/0energy-per-step.png}
  \caption{Average Controller Energy}
  \label{fig:mountaincar_energy}
  \end{figure}

All of the controllers tended to have similar training times when using the same hyperparameters. Figure \ref{fig:mountaincar_train-time} shows the average time it took to train each controller. There is no significant difference between the mean times for each group of controllers. Figure \ref{fig:mountaincar_success} displays the success rate of the controllers based on the ten controllers trained for each element and combination. Of the single element rewards, the speed reward was the only group of controllers to all consistently reach the goal. The velocity reward did not have any successful controllers. All of the combined reward functions, save for the error and goal combination, had consistently successful controllers.\par

 \begin{figure}[H]
  \captionsetup{labelfont=it,justification=centering} 		
  \centering
  \includegraphics[scale=.7]{images/0all_train_time.png}
   \caption{Average Controller Training Time}
   \label{fig:mountaincar_train-time}
 \end{figure}
 
  \begin{figure}[H]
  \captionsetup{labelfont=it,justification=centering} 		
  \centering
  \includegraphics[scale=.7]{images/0success_rate.png}
   \caption{Controller Success Rate}
   \label{fig:mountaincar_success}
 \end{figure}


  
\section{Conclusion}
In this experiment, each element had its own effect on the outcome of the controllers respective to its primary metric as expected. By focusing on specific elements, other metrics are essentially ignored during the learning process. By combining these different metrics into a single function, the policy optimization begins to control for multiple elements simultaneously as it updates its control scheme to maximize reward. This is most notable with the energy and speed reward elements because of their inherent opposing solutions. To increase travel speed in most cases requires an increase in force in the desired direction, and to increase force requires energy. Figure \ref{fig:mountaincar_speed} shows that speed controllers required, on average, less than 100 steps to reach the goal while it took the energy controllers a little less than 800 steps. Focusing on the energy consumption of each controller, the energy controllers use only a fraction of the force per step that the speed controllers use. The combination of all of the elements, $r_a$ shows a low energy usage, low average number of steps to the goal, and is more consistent than both controllers. \par 
\subsection{Step Differentiation}
The speed and goal reward elements do not immediately differentiate between steps. Relying solely on the learning algorithm's ability to explore the operation space, training with these reward functions were originally expected to not produce as consistent results as reward elements that give an updated value after each timestep, like the error or energy functions. The results shown here give a range of consistence, showing that there may be deeper relationships to the learning goal and reward function than merely how often the learning agent sees a change in the results from step to step. \par 
Take the energy reward $r_e$, which is defined in table \ref{table:elements}. The best way to use the least amount of energy is to not exhort any force at all. Given enough time, the algorithm will have come to this conclusion. With the action being determined by a sample of a mean and standard deviation, which was output from the policy based on the position and velocity, it is very unlikely that the given action would be 0. This result will then give negative rewards at each step, similar to the speed reward function. The learning algorithm, as it continues to explore the operation space, will likely optimize toward the solution of reducing the number of timesteps in the episode itself, and using less energy while accomplishing this method can be seen as a secondary priority. \par 
\subsection{Goal Element Combination}
Learning stability in this context refers to the consistency of obtaining results from the learning algorithms based on the reward function more than the entirety of the algorithm itself \cite{bousquet2002stability}. To measure this, the controllers were observed for how often they were able to complete the solution. As seen in figure \ref{fig:mountaincar_success}, the addition of a goal reward tended to stabilize the learning for this case. This is likely because of the subtle balance of the reward element outputs. Having a maximum possible 1000 steps, each reward is scaled to only have the ability to penalize up to a value of 100 in an episode. The goal reward, $r_g$ not only stopped the episode so there could no longer be any other penalties, but also gives the maximum positive reward, making it a clear priority. \par 
\subsection{Alignment Problem}
For this problem, it was intuitively reasoned that the more total velocity the car exhibited, the quicker it would reach the destination considering there were only 2 directions. This prompted break from the generalized view of designing rewards to focus on a probable solution. None of these controllers produced were successful. Recall from table \ref{table:reward_experiments} that $r_v=V^2$. The coefficient $c_v$ was calculated using the maximum speed of the vehicle, $V_{max}= 0.07$, and the maximum number of timesteps in the episode, $n_{ep} =1000$:
\begin{equation}
    \begin{split}
        100 = r_{v_{max}} c_v n_{ep} \\
        100 = V_{max}^2 c_v n_{ep} \\
        c_v = \frac{100}{V_{max}^2 n_{ep}} \\
        c_v \approx 20.5
    \end{split}
\end{equation}

This velocity goal and coefficient became a positive reward for increasing the velocity of the car to earn more reward. Although by increasing this reward meant a faster overall drive in the environment, the incentive to end the episodes is diminished. This reward scheme caused controllers to be developed that edged to the goal line without crossing it so it may continue to gather reward until the episode's time was up. This difference in the designer's desired goal, and the learned behavior to maximize reward is known as the alignment problem. This may have been avoided through modeling the velocity as a penalty that decreases as the speed increases.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 %									 Case Study 2
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Quadrotor UAV Attitude Control}]{Quadrotor UAV Attitude Control}

For the past century flying vehicles have advanced modern life from how war is fought to how video is captured. As technology has proceeded to become smaller and more powerful, flying vehicles have evolved to smaller more capable vehicles without the need for an on-board human pilot. These unmanned aerial vehicles (UAVs) have the advantage of keeping pilots out of dangerous environments, but with the costs of teleoperation. One vehicle which has become extremely popular within the past several decades is the quadrotor UAV, also referred to as a quadcopter. With vertical take-off and landing, 6 degrees of freedom, and mechanically simple hardware, the quadrotor UAV has been adapted for use in many fields, including commercial inspection \cite{nikolic2013uav}, aerial photography, and personal or spectator entertainment.

\section{Quadcopter}
This aerial vehicle consists of four propeller-equipped motors that receive power signals from a flight controller to control the amount of thrust produced. The combination and ratio of thrust between motors give it the ability to maneuver in a three dimensional space by changing its orientation to apply force in many directions. The speed of the motors are controlled on the electronic speed controllers (ESCs) that receive the desired thrust signals from the flight controller in the center. Sensors are equipped on the flight controller which may measure the UAV's velocity, direction, attitude, and even its position, given the right environments. These sensors will be used to assess the state of the quadrotor at any time. The flight controller uses this perception to regulate the control signals translated to the ESCs to send to the motors.\par
 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
 \centering
 \includegraphics[scale=.4]{images/quad_axis.png}
  \caption{Quadcopter diagram (left) and axis of rotation (right)}
  \label{fig:axis}
  \end{figure}
A typical control hierarchy for the operation of a quadcopter is represented in figure \ref{fig:hierarchy}. With the appropriate sensors, a high-level task, e.g. flying a specific trajectory, can be obtained by controlling the position, angle, angular velocity rates, and the thrust producing motor velocities \cite{lupashin2014platform}. A cascading controller such as this
 \begin{figure}[H]
 \captionsetup{labelfont=it,justification=centering} 		%%% Puts label font in italics, must be done for all figures %%%
 \centering
 \includegraphics[scale=.5]{images/ctr_tot.png}
  \caption{Quadcopter control hierarchy}
  \label{fig:hierarchy}
  \end{figure}

\section{GymFC}
The GymFC environment \cite{1804.04154} was created using the framework of OpenAI's Gym test bench software. Although Gym is efficient for studying algorithms, these environments, mostly toy problems and theoretical test benches, are not designed for applying the resulting policies to real-world problems. For application purposes OpenAI has included the ability to create and register environments using their framework, like GymFC. Koch used OpenAI's framework to create an environment specifically for training a flight controller for attitude control of a quadrotor UAV. By incorporating a simulation envoronment and an available quadcopter model, the envoronment can directly take motor commands, simulate the output, and give an evaluation of the current state with a reward signal. \par 
GymFC uses four PWM signals for a quadcopter's motors as actions, an error measurement of the quadcopter's angular velocity along with the angular velocities of the individual motors as state observations, and a negative clipped aggregation of the velocity errors as a reward signal. 
\begin{equation}
    r_t = -clip(sum( |\Omega_t^* - \Omega_t |) / 3\Omega_{max})
\end{equation}
The difference between the target angular velocities, \(\Omega_t^*\), and the current angular velocities, \(\Omega_t\) are divided by three times the upper bound of the target velocity range, \(\Omega_{max}\) that the target velocities are sampled from. This value is clipped between 0 and 1, and is reflective of the angular velocity errors. By maximizing this reward, or reducing this penalty, the error is effectively being decreased.
To decrease the oscillation of the output signal and decrease the output of the control signal, the reward function was shaped to include two more terms: \(r_\Delta\) and \(r_y\) \cite{koch2019neuroflight}.
\begin{equation}
    r_\Delta = \beta \sum_{i=0}^{N-1}max\{0,\Delta y_{max}-(\Delta y_i)^2\}
\end{equation}
\begin{equation}
    r_y = \alpha (1-\overline y)
\end{equation}
\(\beta\) and \(\alpha\) are scaling constants for the reward \(r_\Delta \) and \(r_y\) reward signals respectively.  \(r_\Delta\) is only active when in the threshold of angular velocity error of each angle. While in this region, the difference of the previous action, \(y_{t-1}\), and current action, \(y_t\), are squared and subtracted from a constant,  \(\Delta y_{max}\), which has been set as the maximum allowable change. This value is capped at 0 so that the reward is only given when the change is higher than the allowable change. The second added term, \(r_y\), uses one subtracted by the mean of the action, \(\overline{y}\), as a reward for using smaller actions while approaching the target velocities. \par

\section{Simulator}
\subsection{Quadrotor Model}
\section{Experiment}
\section{Results}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 %									 Conclusion and Future Works
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter[\normalfont{Conclusion and Future Works}]{Conclusion and Future Works}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%								        REFERENCES!!! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of references, references must be consistent with corresponding bib.tex file %%%
\printbibliography

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%										 APPENDICES!!!   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%----------OPTIONAL WRITINGS------------%%%
%--Autonomy--%
%Humans have increased productivity and efficiency by using machines to automate tasks for centuries. These machines provided physical power, calculations, and other services that saved time, energy and risks, allowing higher production for lower costs. As technology improves, automating tasks has become more accessible, leading to the exploration of automating more complex tasks like processing language, visual classification, or operating intricate vehicles. In performing these tasks, multi-dimensional information with sometimes unknown correlations must be organized and processed by a computer. When automating a task, important features must be established. Although there are techniques to formalize the feature selection process, experts do not always agree on the results the analysis and may also overlook important system features. In order to address these challenges, for the past several decades artificial intelligence has been researched and developed to automate the expert analysis. Much of this research has focused on using machine learning, a branch of artificial intelligence, to process the complex information with numerical reasoning, and provide automated decisions.
%--Autonomy--%

%-- Human Aggressive Control Intro --%
%Man has always been obsessed with their domination over every aspect of life. A mixture of personal desire and necessary survival, this urgency for knowledge and power has fueled many advances throughout recorded and assumed history. Within the last few centuries, the fundamentals of control has been formalized through research and application. By studying a system, its functionalities may be understood at a level that could lend itself to manipulation for an exacted purpose. This manipulation is important, not only for desired gains, but also for safety, efficiency, reliability, and longevity of operation. \par
%-- Human Aggressive Control Intro --%

%Although systems may be examined with scrupulous detail, its definition may still be incomplete, inaccurate, or described as so complex that its control is nearly insuperable.

%--RL used for Control--%
%Reinforcement learning may be used to discover a control strategy without directly creating a behavior model and may lead to controller creation that is robust and accurate. By interacting with the system through the controllable elements and modifying its action policy based on the system outputs a learning agent learns how to control the system with experience. To represent the controls problem as a reinforcement learning problem, the important learning parameters must be redefined in the scope of controls problems.\par
%--RL used for Control--%
%%%----------OPTIONAL WRITINGS------------%%%


\end{document}   %%% marks the end of the document %%%
